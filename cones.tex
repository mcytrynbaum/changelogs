\documentclass[11pt,reqno]{amsart}
\usepackage{amssymb,mathrsfs,color}
\usepackage{pinlabel}
\usepackage{graphicx}
\usepackage{graphics} 
\usepackage{algorithm}
\usepackage{algorithmic}


\graphicspath{ {c:/users/mcytrynbaum/documents/rfigures/} {c:/users/mcytrynbaum/Desktop/package/} }
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\usepackage{amsmath} % for all math functions and operations
\usepackage{amsfonts} % use this to write scripts (e.g. Real nums, etc)
\usepackage{mathtools} %for other math stuff not included in packages above
\usepackage{amsthm} % in case you want the THM: COR: LEMMA: setup
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry} %for setting the margins

%\setlength\parindent{0pt}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{examp}[thm]{Example}
\newtheorem{remark}[thm]{Remark}
\setcounter{equation}{0}
\numberwithin{equation}{section}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment[1]}{ \hfill // \eqparbox{}{#1} }
\newcommand{\prf}{\begin{proof}}
\newcommand{\eprf}{\end{proof}}
\newcommand{\lft}{\left(}
\newcommand{\rt}{\right)}
\newcommand{\be}{\beta}
\newcommand{\eps}{\epsilon}
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}
\newcommand{\al}{\alpha}
\newcommand{\bp}{\begin{pmatrix}}
\newcommand{\ep}{\end{pmatrix}}
\newcommand{\inv}{^{-1}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\corr}{\text{Corr}}
\newcommand{\ssumi}{\sum_{i=1}^n}
\newcommand{\ssumj}{\sum_{j=1}^n}
\newcommand{\ssumk}{\sum_{k=1}^n}
\newcommand{\im}{\text{Im}}
\newcommand{\mc}{\mathcal}
\newcommand{\mr}{\mathbb{R}}
\newcommand{\ol}{\overline}
\newcommand{\ul}{\underline}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\ital}{\emph}
\newcommand{\tb}{\textbf}
\newcommand{\pa}{\partial}
\newcommand{\et}{\eta}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\lag}{\langle}
\newcommand{\rag}{\rangle}

\newcommand{\pre}{\phi}
\newcommand{\econ}{e}
\newcommand{\coordpre}{\mathrm{CP}}
\newcommand{\prealloc}{(2^X)^A}
\newcommand{\sub}{\subseteq}
\newcommand{\strcore}{\mathrm{SC}(X,U)}
\newcommand{\core}{\mathrm{C}(X,U)}
\newcommand{\stable}{\mathrm{S}(X,U)}
\newcommand{\fecon}{E}
\newcommand{\fix}{\mathcal{E}}
\newcommand{\suq}{\succeq}
\newcommand{\peq}{\preceq}
\newcommand{\su}{\succ}
\newcommand{\pe}{\prec}
\newcommand{\toppre}{\ol{\pre}}
\newcommand{\bopre}{\ul{\pre}}
\newcommand{\strongc}{\mathcal{G}}
\newcommand{\strongcomp}{S}
\newcommand{\acto}{Q_0} 
\newcommand{\actok}{Q_0^k} 
\newcommand{\actak}{Q_A^k} 
\newcommand{\acta}{Q_A} 
\newcommand{\actc}{Q_C} 
\newcommand{\act}{Q} 
\newcommand{\actt}{Q_{temp}} 
\newcommand{\actacum}{\wh{Q_A}} 
\newcommand{\actocum}{\wh{Q_0}} 
\newcommand{\disto}{d}
\newcommand{\distt}{D}
\newcommand{\preo}{\pre^{0}} 
\newcommand{\pref}{\pre^{f}} 
\newcommand{\coll}{I}
\newcommand{\collk}{I^k}
\newcommand{\res}{\act_{int}}
\newcommand{\reach}{H}
\newcommand{\forest}{F}
\newcommand{\fixfind}{\mathcal{E}}
\newcommand{\fixtemp}{\mathcal{E}'}
\newcommand{\stp}{\mathrm{STOP}}
\newcommand{\pair}{(F,I)}
\newcommand{\roott}{R}
\newcommand{\depth}{D}
\newcommand{\unproc}{\mathcal{J}_u}

\title{Using Lattice Geometry to Find All Stable Allocations}
\author{Max Cytrynbaum and Scott Duke Kominers}

\begin{document}
\maketitle

We consider a finite set of contracts $X$, each of which is associated with at least one agent $a\in A$. 
We call a subset $Y\sub X$ an \emph{allocation}, and let $2^X$ denote the set of all allocations. 
Let $d(x)$ be the set agents associated with a contract $x\in X$, and extend this definition to allocations by writing $d(Y) \equiv \bigcup_{y\in Y} d(y)$.
Note that, in general, we may have $|d(x)| > 2$ for multilateral contracts.

For $a\in A$, we let $Y_a = \{y\in Y: \, a\in d(y)\}$ denote the set of contracts associated with that agent (note that we may have $Y_a = \emptyset$). Thus, $2^{X_a}$ denotes the set of all allocations naming an agent $a\in A$. 
We assume that each $a\in A$ has strict preferences over $Y \in 2^{X_a}$, where the utility of an allocation is given by the one-to-one function $U_a: 2^{X_a} \to \mr$.
Let $\su_a$ denote the strict preference relation induced by these utility functions over bundles $Y \in 2^{X_a}$, with $\suq_a$ denoting the weak relation. Thus, $Y \suq_a  Z \iff$ $Y \su_a Z$ or $Y = Z$.

We define choice functions in the usual way for $Y\in 2^{X_a}$ by 
\[
C_a(Y) = \argmax_{Z\subseteq Y} \, U_a(Z)
\]
and, by an abuse of notation, extend choice functions to $Y\in 2^X$ by setting $C_a(Y) \equiv C_a(Y_a)$.
Throughout the matching portion of this paper, we will assume that $\emptyset_a \in X$ for all $a \in A$, where $\emptyset_a$ denotes $a$ being unmatched. Note that $d(\emptyset_a) = \{a\}$. 
\section{Finding All Stable Matchings} 
In this section, we illustrate the technique in the classical Hatfield and Milgrom model -- literature, etc... 
\subsection{Model and Notation} 
Let $X = D\times H$, where we think of $D$ as ``doctors'' and $H$ as ``Hospitals''. We may think of the set of contracts as $X = D\times H \times E$, where $E$ is a finite set of contract terms; for instance, $E$ could be a set of wages.

An allocation $Y$ is said to be \emph{stable} if it is 
\begin{enumerate}
\item \emph{individually rational} - $C_a(Y) = Y_a$ for all $a \in A$
\item \emph{unblocked} - There is no allocation $Z \not = \emptyset$ such that $Z_b \sub C_b(Y\cup Z)$ for all $b \in d(Z)$. 
\end{enumerate}
We let $S(X,U)$ denote the set of all stable matchings.

\section{Finding All Strict Core Matchings}
Talk about Echenique's contribution and what we are going to do, strict core is equivalent to...
Note how incredibly general our construction is - multilateral discrete matching in networks, essentially no structure, fully manipulates lattice structure of strict core matchings.   
\subsection{Model and Notation}
We construct an appropriate framework in which to generalize the fixed point construction in Echenique and Yenmez. 
We start by generalizing the classical notion of a \emph{prematching}. 

\begin{defn}[Preallocation] We call a map $\pre: A \to 2^X$ a \emph{preallocation} if $\pre(a) \in 2^{X_a}$ for all $a\in A$. Let $(2^X)^A$ denote the set of all preallocations.  
\end{defn}

Intuitively, a preallocation assigns each agent to a bundle of contracts naming him or her. We can think of $\pre(a)$ as the set of contracts ``held'' by agent $a$.   

We can associate each allocation $Y\subseteq X$ with a unique preallocation $\pre_Y$ in a natural way by setting $\pre_Y(a) = Y_a$.
\begin{remark} Note, however, that not all preallocations can be derived from allocations in this way.
For example, consider the case where $\emptyset \not = \pre(a)_b \not = \pre(b)_a$. In the preallocation $\pre$, $a$ holds contracts naming $b$ that are \emph{not} in the bundle of contracts held by $b$ naming $a$.  
In particular, there does not exist an allocation $Y$ such that $\pre = \pre_Y$. 
\end{remark}

With this example in mind, we say that $\pre \in \prealloc$ is a \emph{coordinated} preallocation if there exists an allocation $Y$ such that $\pre = \pre_Y$. 
We denote the set of all cooordinated preallocations by $\coordpre\sub \prealloc$.

An allocation $Y \sub X$ is said to be in the \emph{strict core} if there does not exist a blocking set $Z \sub X$ such that 
\[
U_b(Z_b) \geq U_b(Y_b) \qquad  \forall b\in d(Z)
\]
where \emph{at least one} of the above inequalities holds strictly. We denote the strict core by $SC(X,U)$. 
\subsection{Fixed Preallocations and the Strict Core}
Our method proceeds by identifying allocations $Y \in SC(X,U)$ with fixed points of an operator on preallocations, generalizing the construction in Echenique and Yenmez. 

For each agent $a \in A$, we define 

\[
V(\pre, a) = \{Z \in 2^{X_a}: \exists Y \in 2^X \, s.t. \,  Y_a = Z, \, Y_b \su_b \pre(b) \: \forall b \in d(Y)\setminus\{a\} \}
\]

Intuitively, $V(\pre, a)$ is the \emph{possibility set} for an agent $a$ at a preallocation $\pre$. 
It contains all sets of contracts naming $a$ that are part of a larger economy $Y$ where every other agent $b \in d(Y)$ weakly prefers their contracts under $Y$ to their contracts under the pre-allocation $\pre$.  

Next, we define an operator $T: \prealloc \to \prealloc$ by setting $T \pre(a) = \max V(\pre, a)$, where the maximum is taken under the preference relation $\suq_a$ for each $a \in A$.
Note that $\emptyset_a \in V(\pre,a)$ for any $\pre \in \prealloc$, so $T$ is well-defined. Let $\fix(T)$ denote the fixed points of $T$. Define $\fecon(T) = \{Y \in 2^X: \, \pre_Y \in \fix(T)\}$, the collection of allocations $Y$ whose corresponding preallocation $\pre_Y$ is fixed by $T$.

Before our first result, we note a simple fact: if $\pre \in \coordpre$, then $\pre(a) \in V(\pre, a)$.
To see this, note that $\pre \in \coordpre$ means that there exists $Y \sub X$ with $\pre = \pre_Y$. 
Then $Y$ is an allocation satisfying the conditions in $V(\pre_Y,a)$, so that $Y_a  = \pre(a) \in V(\pre,a)$. 
With the definitions above, we have a simple result
\begin{lemma} $\fecon(T) = \strcore$
\end{lemma}
\prf
First, suppose that $Y \not \in \strcore$. Then by definition, there exists some blocking allocation $\emptyset \not = Z \sub X$ such $Z_b \suq_b Y_b$ for all $b \in d(Z)$.
Let $a \in d(Z)$ be such that the inequality above is strict, and consider $\pre = \pre_Y$. 
In particular, $Z_b \suq_b \pre_Y(b)$ for all $b \in d(Z) \setminus \{a\}$, so that $Z_a \in V(\pre_Y, a)$.
Then $T \pre_Y(a) = \max V(\pre_Y, a) \suq_a Z_a \su_a Y_a = \pre_Y(a)$, so $T\pre_Y(a) \not = \pre_Y(a)$, and $Y \not \in \fecon(T)$.  

Suppose, conversely, that $Y \not \in \fecon(T)$ so that $\pre_Y \not \in \fix(T)$.  
Then there exists an agent $a \in A$ such that $T\pre_Y(a) = Z_a \not = \pre_Y(a)$ for some allocation $Z$.  
By the definition of $V(\pre_Y,a)$, we have $Z_b \suq_b \pre_Y(b) = Y_b$ for $b \in d(Z) \setminus \{a\}$.
We know $\pre_Y$ is coordinated, so by the simple fact above $\pre_Y(a) \in V(\pre_Y,a)$.
Then $Z_a = T \pre_Y(a) \su_a \pre_Y(a) =  Y_a$. Then $Z$ is a blocking coalition for $Y$, so $Y \not \in \strcore$.  
\eprf
Thus, we have identified $\strcore$ with the set of \emph{coordinated} preallocations $\pre \in \coordpre$ such that $T \pre = \pre$. 
This result shows that an algorithm that finds all $\pre \in \fix(T)$ will also find all strict core matchings. 

However, if there are \emph{uncoordinated} preallocations that are also fixed by $T$, such an algorithm may return extraneous solutions not associated with any strict core matching.
The following lemma, which is essential for the construction of our algorithm, shows that there are no such preallocations.
Note that this result significantly generalizes the corresponding lemma in Echenique and Yenmez and also subsumes the main theorem of Kojima (xxxx). 

\begin{lemma} $\fix(T) \sub \coordpre$
\end{lemma}
\prf
We begin with an important fact that will be used repeatedly.
Suppose that $\pre \in \fix(T)$. Then for any $a \in A$, we have $\pre(a) = T\pre(a) \in V(\pre,a)$.
Thus, there exists an allocation $Y$ such that $Y_a = \pre(a)$, and $Y_b \suq_b \pre(b)$ for all $b \in d(Y) \setminus \{a\}$.
Since $Y_a = \pre(a)$, then in fact $Y_b \suq_b \pre(b)$ holds \emph{for all} agents $b \in d(Y)$. 
For any $b \in d(Y)$, $Y$ then satisfies the conditions in the definition of $V(\pre,b)$, so $Y_b \in V(\pre,b)$. 
Therefore, $\pre(b) = T\pre(b) \suq Y_b \suq \pre(b)$, so equality holds throughout. In particular, $Y_b = \pre(b)$ for all $b \in d(Y)$. 

Fix $a_1 \in A$. Since $\pre \in \fix(T)$, the argument above shows that there exists an allocation $Y$ such that $Y_{a_1} = \pre(a_1)$, and, in particular, $Y_b = \pre(b)$ for all $b \in d(Y)$. 
Therefore, $U(\pre,a_1) = \{Y \in 2^X: Y_{a_1} = \pre(a_1), \: Y_b \suq \pre(b)\: \forall b \in d(Y)\setminus \{a_1\}\}$, the collection of global allocations available to $a_1$ at $\pre$, is non-empty, so there exists an allocation 

\[
Y \in \argmax_{Z \in U(\pre,a_1)} |d(Z)|
\]

Let $A_1 = d(Y)$. If $A_1 = A$, we are done, since then by the construction above $Y_a = \pre(a)$ for all $a \in A$, so $\pre = \pre_Y$ and $\pre \in \coordpre$. 

Then assume that $A_1 \not = A$, and pick $a_2 \in A \setminus A_1$. By the fact at the beginning of the proof, there exists an allocation $Z$ such that $Z_{a_2} = \pre(a_2)$, and, in fact, $\pre(b) = Z_b$ for all $b \in d(Z)$. 
Define $A_2 = d(Z) \cap A_1^c$, which is non-empty by construction.
Let $b \in A_2$. We will show that $d(\pre(b)) \cap A_1 = \emptyset$.
That is, under the preallocation $\pre$, agent $b \in A_2$ is \emph{not} holding any contracts that name agents in $A_1$. 

Suppose not, so there exists $c \in A_1 \cap d(\pre(b))$.
Then, in particular, $c \in d(\pre(b)) = d(Z_b) \sub d(Z)$, so applying the fact proved at the beginning, $Z_c = \pre(c) = Y_c$. 
Since $c \in d(Z_b)$, there exists a contract $z \in Z_c$ naming both $c$ and $b$.
Then $b \in Z_c = Y_c$, so $b \in d(Y_c) \sub d(Y) = A_1$, so $b \in A_1 \cap A_2 = \emptyset$. 
This is a contradiction, so it must be the case that $d(\pre(b)) \cap A_1 = \emptyset$ for all $b \in A_2$. 

Define $S = \bigcup_{b \in A_2} \pre(b)$. We have just shown that $d(S) \sub A_1^c$.
We also have $d(S) = \bigcup_{b \in A_2} d(\pre(b)) = \bigcup_{b \in A_2} d(Z_b) \sub d(Z)$, so $d(S) \sub A_1^c \cap d(Z) = A_2$.
Clearly $b \in d(\pre(b))$ for all $b \in A_2$, so $A_2 \sub d(S)$. Then $A_2 = d(S)$. 

Set $W = Y \cup S$. We have now shown that $A_2 \not = \emptyset$ and $A_1 \cap A_2 = \emptyset$.
Since $d(Y) = A_1$ and $d(W) = A_2$, it follows that $W \cap Y = \emptyset$, so we have  

\begin{enumerate}
\item $W_b = Y_b = \pre(b)$ for all $b \in A_1$; in particular, $W_{a_1} = \pre(a_1)$. 
\item $W_b = S_b = \pre(b)$ for all $b \in A_2$.
\end{enumerate}

Then apparently $W \in U(\pre,a_1)$ as defined above. However, by construction $|d(W)| > |d(Y)|$, which contradicts our original choice of $Y$.
This finishes the proof.
\eprf
\subsubsection{Discussion}
Combining these lemmas, we see that searching for strict core allocations in a very general model of multilateral matching with contracts is equivalent to searching for the fixed points of $T$. 
Our algorithm depends heavily on this result, which shows, critically, that the fixed points $\fix(T)$ are only as dense in $\prealloc$ as the strict core outcomes.

Our maximal domain results will show that this is not the case for the natural extension of this method to \emph{true} core outcomes $\core$.
For true core outcomes, where the lattice algorithm fails, $T$ also fixes at a large number of extraneous, uncoordinated preallocations. 
\subsection{The Lattice of Fixed Preallocations}
In this section, we generalize constructions from Echenqiue and Yenmez showing the the fixed points of the squared operator $T^2$ form a lattice. 
First, we define a natural partial order on the set of preallocations $\prealloc$.

Say that $\pre \su \pre'$ if and only if $\pre(a) \suq_a \pre'(a)$ for all $a \in A$, where at least one of these inequalities \emph{holds strictly}. 
Thus, we write $\pre \suq \pre'$ if and only if $\pre \su \pre'$ or $\pre = \pre'$. 
This is a product order on a product space, which makes $\prealloc$ into a complete lattice (cite)(footnote about joins and meets). 
Next, we give a suquence of results concerning the operator $T$ and its fixed points. 
These results are an almost direct extension Lemmas 4.x through 4.y of Echenique and Yenmez.
For convenience, we reproduce the first result in our framework - the rest follow from work in Echenique in and Yenmez. 
\begin{lemma} $T$ is antitone 
\end{lemma}
\prf
Let $\pre \leq \pre'$ be preallocations.
Fix $a \in A$, and let $Z \in V(\pre',a)$.
Then there is an allocation $Y \sub X$ with $Y_a = Z$ such that $Y_b \suq_b \pre'(b)$ for $b \in d(Y) \setminus \{a\}$.
Then $Y_b \suq_b \pre'(b) \suq_b \pre_b$ also for all such agents, so we also have $Z \in V(\pre,a)$.
Then $V(\pre,a) \supseteq V(\pre',a)$, so that $T\pre(a) \suq T\pre'(a)$. $a$ was arbitrary, so $T\pre \suq T\pre'$ under our partial order. 
\eprf
The following lemmas follow exactly as in Echenique and Yenmez, using the antitonicity of $T$. 
\begin{cor} $T^2$ is isotone, and $\fix(T^2)$ is a non-empty complete lattice. 
\end{cor}
\begin{lemma} No two preallocations $\pre$ and $\pre'$ can be compared under the partial order on $\prealloc$.
\end{lemma}
\begin{lemma} There exist preallocations $\ol{\pre}$ and $\ul{\pre}$ such that for all $\pre \in \fix(T)$, we have $\ol{\pre} \suq \pre \suq \ul{\pre}$. Moreover, if $\pre = \ol{\pre}$ or $\pre = \ul{\pre}$, then $\fix(T) = \{\pre\}$. 
\end{lemma}

\subsection{Exploiting Lattice Geometry to Find All Strict Core Allocations}
\subsubsection{Introduction}
In this section, we give an algorithm that finds \emph{all} strict core allocations in the model of multilateral matching with contracts considered above.
Our algorithm builds upon the original approach in Echenique and Yenmez xxxxx.
Modifying Echenique and Yenmez's original algorithm, we show how to fully exploit the geometric structure of the problem to efficiently find the full set of strict core matchings. 

At a basic level, the algorithm proceeds by successively initializing modified versions of the original matching problem, in which each agent has a truncated preference list.
Our contribution is to realize that many of the subproblems created while we search the lattice $\fix(T^2)$  are either completely redundant or share information with other subproblems.
Using cone geometry, we show how to efficiently combine this shared information to more quickly identify the strict core matchings. 

From a computational complexity perspective, evaluating the $T$ operator is equivalent to checking whether or not a specific allocation $Y$ is in the strict core.
This operation can be very costly even in classical matching models\footnote{add some concrete complexity reference}.
Our algorithm attempts to minimize the number of evaluations of $T^2$ by using lattice structure to combine geometric information from related subproblems. 

\subsubsection{Notation and Intuition} 
Let $\pre^*$ denote the largest preallocation under the partial order $\su$.
That is, $\pre^*(a) = \argmax U_a(\pre(a))$ for each $a \in A$.
Suppose that the total number of agents $|A| = m > 0$.
We will use $\langle \pre \rangle$ or $\langle \pre(1) \hdots \pre(m) \rangle$  to denote a version of the original problem, in which each agent $a$'s preference list is truncated to allocations in $2^{X_a}$ ranked below $\pre(a)$ by agent $a$ (inclusive). 
Often in what follows, we will identify a problem $\lag \pre \rag$ with its maximal point $\pre$.
Let $\fix(\pre) \equiv \{\pre' \in \fix(T): \: \pre' \peq \pre\}$ denote the fixed points of $T$ below $\pre$.  

By Tarski's Theorem\footnote{add standard Tarski argument}, the sequence $(T^2)^k \pre^*$ converges to a preallocation $\toppre$, fixed by $T^2$, which is the maximal point of the lattice $\fix(T^2)$.
If $T \toppre = \toppre$, then by lemma xxxx, $\fix(T) = \{\toppre\}$.
So suppose that $T \toppre \not = \toppre$. 
Clearly $\fix(T) \sub \fix(T^2)$ for any operator, and, by iteratively applying $T^2$, we have learned that $\fix(T) \sub \fix (T^2) \sub \{\pre': \pre' \peq \toppre\} \equiv \{\pre' \peq \toppre\}$. We will often denote cone relations of this type by $\fix(T) \peq \toppre$. 

Since $T\toppre \not = \toppre$, apparently $\fix(T) \sub \bigcup_{i = 1}^m \{\pre' \peq \toppre - e_i\}$, where $e_i$ denotes the standard unit vector in $\mr^m$ \footnote{identifying $\prealloc$ with a grid in $\mr^m$ as above.}. 
Consider a subproblem of the form $\lag \toppre - e_i \rag$, and let $T_i$ be the operator corresponding to this subproblem, where agents' preferences are truncated above $\toppre - e_i$.
The key insight of Echenique (2003), which generalizes to the current model, is that $\fix(T) \cap \{\pre' \leq \toppre - e_i\} \sub \fix(T_i)$.
That is, the fixed points of $T$ contained in the cone $\{\pre' \peq \toppre - e_i\}$ are also fixed points of the subproblem $\lag \toppre - e_i \rag$ with operator $T_i$. 
Then apparently we have $\fix(T) \cap \{\pre' \leq \toppre - e_i\} \sub \fix(T_i) \sub \fix(T_i^2)$, so information about the lattice of fixed points of $T_i^2$ can be used to bound $\fix(T)$.
The general result in our setting is as follows 
\begin{lemma} \label{lemma:contain}
Suppose $\pre \peq \wh{\pre}$, and let $\wh{T}$ denote the strict core operator for the problem $\lag \wh{\pre} \rag$.
If $\pre \in \fix(T)$, then $\pre \in \fix(\wh{T})$.
In particular, $\fix(T) \cap \{\pre \leq \wh{\pre}\} \sub \fix(\wh{T}^2)$. 
\end{lemma}
\prf
Let $\wh{V}(\pre, a)$ be the defining set for $\wh{T}$ in the subproblem.
Then for any $a \in A$ and $\pre \in \prealloc$ we have 
$\wh{V}(\pre,a)  = \{Z \in 2^{X_a}: \exists Y \in 2^X \, s.t. \,  Y_a = Z, \, \wh{\pre}(b) \suq_b Y_b \suq_b \pre(b) \: \forall b \in d(Y)\setminus\{a\} \}$
then clearly $\wh{V}(\pre,a) \sub V(\pre,a)$.  
We can compute $\wh{T}\pre(a) = \max \wh{V}(\pre,a) \leq \max V(\pre,a) = T\pre(a)$ for any preallocation $\pre$. 

Now, suppose that $\pre \in \fix(T)\cap\{\pre' \peq \wh{\pre}\}$.
Then we have have $\pre(a) = T\pre(a) \suq_a \wh{T}\pre(a)$ for all $a \in A$. 
By lemma xxxx above, $\fix(T) \sub \coordpre$, so $\pre = \pre_Y$ for some allocation $Y \in 2^X$.
In particular, $Y_a = \pre(a) \peq_a \wh{\pre}(a)$, so $\pre(a) \in \wh{V}(\pre,a)$.
Then by the definition of $\wh{T}$, we get $\wh{T}\pre(a) \suq_a \pre(a)$. 

Combining this with the statement above, we have $\pre(a) = \wh{T} \pre(a)$, so $\pre \in \fix(\wh{T})$. 
\eprf 
Returning to the discussion above, we showed that $\fix(T) \sub \bigcup_{i = 1}^m \{\pre' \peq \toppre - e_i\}$, a union of cones in the lattice of preallocations. 
That is, $\fix(T) \sub \bigcup_{i = 1}^m \fix(\toppre - e_i)$.
Fix $i$ and consider $\fix(\toppre - e_i)$, the fixed points of $T$ below $\toppre - e_i$.
For convenience, denote $S_i \equiv T_i^2$ and $\toppre_i \equiv \toppre - e_i$.
Starting with $\toppre_i$, the unanimously most preferred preallocation in the subproblem $\lag \toppre_i \rag$, lemma xxxx and the Tarski argument used above show that each iteration of $S_i$ gives a ``cone guarantee'' on the fixed points $\fix(T)$ of the form $\fix(\toppre_i) \peq S_i^k \toppre_i$.

Pursuing this strategy for $i = 1 \hdots m$, we can bound the complete set of fixed points $\fix(T)$ in a union of subproblem cones. 
We continue this strategy recursively by generating $m$ new subproblems whenever a monotonic sequence of the form $S_i^k \toppre_i $ stops at a fixed point of $S_i^k$. 
In this way, we can eventually find the full set of strict core outcomes, directly extending the algorithm of Echenique and Yenmez to the present setting.  

Depending on the structure of the lattice $\fix(T^2)$, the strategy described above can actually be quite similar to greedy search.  
In particular, the number of subproblems generated at the kth recursive level scales exponentially in $k$ as $|A|^k$.

As we will show, many of the subproblems generated by this strategy are either redundant or do not need to be solved completely.
In particular, we can significantly improve this algorithm by capitalizing on the structure of the preallocation lattice, which allows subproblems to share geometric information with each other. 
With this modification, evaluation of the full Tarski sequence $S_i^k \toppre_i$ is often unnecessary. 
In fact, it is often the case that subproblems can be stopped or removed entirely after a few iterations. 
We will illustrate this idea with a couple of simple examples.  

First, we recall some graph-theoretic notation, which will be useful in the coming sections. A \emph{graph} is a pair $(V,E)$, where $V$ is a collection of \emph{vertices} and $E \sub V \times V$ is a collection of \emph{edges}.
In a \emph{directed graph} (digraph), the order of vertices in an edge matters. 
A \emph{directed path} is a sequence $e^1 \hdots e^n$ of edges, where $e_2^k = e_1^{k+1}$ for $k \leq n-1$. 
A subset $W \sub V$ is \emph{strongly connected} if for any $v_i, v_j \in W$, there exist directed paths $v_i \to v_j$ and $v_j \to v_i$ using only vertices in $W$. 
Strong connectedness is an equivalence relation on $V$, and the (disjoint) maximal strongly connected subsets of $V$ are called the \emph{strong components} of $G$, which we denote by $\strongc$. 
Weak connectedness is defined similarly for an undirected graph, where an (undirected) path is defined as above, ignoring the order on vertices in an edge. 
The \emph{reachable set} from a vertex $v$ is the set of all $v' \in V$ such that there exists a directed path $v \to v'$. 

By a \emph{rooted tree}, we mean a graph that is connected with no cycles (paths starting and ending at the same vertex) and has one vertex designated as the \emph{root}. 
Edges on a rooted tree are naturally oriented away from the root. 
A collection of such rooted trees is called a \emph{forest} and, for such a collection, for a vertex $w$ we let $r(w)$ denote the root of the tree containing $w$, and we let $c(w)$ denote the \emph{children} of $w$ as above - the vertices connected by an edge to $w$ on a path away from the root - and $p(w)$ denote the parent vertex of $w$, defined similarly.  

\subsubsection{Example 1 - Colliding Subproblem Cones}
In this section, we give a simple example of how subproblems can share information.
For easy of visualization, consider the case $|A| = 2$. As noted previously, we may identify $\prealloc$ with a grid, in this case in $\mr^2$.
As above, we iterate $T^2$ to find $\toppre = \max \fix(T^2)$, which we identify, for instance, with the integer lattice point $(10,10) = \toppre$. 
Using the notation above, there are isotone operators $S_1 = T_1^2$ and $S_2 = T_2^2$ corresponding to the subproblems $\lag \toppre - e_1 \rag$ and $\lag \toppre - e_2 \rag$, respectively. 

Beginning with subproblem 1, suppose that $S_1 (9,10) = (8,10)$, $S_1^2 (9,10) = (7,10)$, and $S_1^3 (9,10) = (7,9)$.
By our arguments above, we now have a cone guarantee on the fixed points in the first subproblem; specifically, $\fix(T) \cap \{(x,y) \peq (9,10)\} \peq (7,9) \peq (10,9) = \toppre_2$.
Therefore, $\fix(\toppre_1) = \fix(T) \cap \{(x,y) \peq (9,10)\} \sub \fix(T) \cap \{(x,y) \peq (10,9)\} = \fix(\toppre_2)$. 
That is, the solutions to subproblem 1 - the fixed points of $T$ in $\{(x,y) \peq (9,10)\}$ - are \emph{contained} in the set of solutions to subproblem 2.  

Now suppose that we stop working on subproblem 1 and begin iterating with $S_2$.
Suppose that $S_2(10,9) = (10,6) = S_2^2(10,9)$, so the sequence stops.
Then we have learned that $\fix(\toppre_1) \sub \fix(\toppre_2) \peq (10,6)$. 
However, by iterating $S_1$, we also learned that $\fix(\toppre_1) \peq (7,9)$. 
Then apparently $\fix(\toppre_1)$ is contained in the cone intersection $\{(x,y) \peq (7,9)\} \cap \{(x,y) \peq (10, 6)\} = \{(x,y) \peq (7,6) \}$. 
That is, by \emph{combining the information} from subproblem 2 with subproblem 1, we have learned that $\fix(\toppre_1) \peq (7,6)$. 

/

////// INSERT FIGURE - standard 2-dim cone drawing  //////

/

Note that, even if we learn $\fix(\toppre_1) \sub \fix(\toppre_2)$, we may still wish to retain the information associated with subproblem 1. 
If, for instance, the outcome for subbproblem 2 was instead that $S_2(10,9) =  = (9,6) = S_2^2(10,9)$, then we would also know that $\fix(\toppre_2) \sub \fix(\toppre_1)$, so that $\fix(\toppre_2) \peq \min((9,6),(7,9)) = (7,6)$ and, in fact, $\fix(\toppre_1) = \fix(\toppre_2)$.
The idea of equivalent subproblems is explored further in the next example. 

\subsubsection{Example 2 - The Collision Graph}
In this section, we give intuition for how to efficiently combine information and track the relations between different subproblem cones. 
Consider the case $|A| = 3$, where we identify $(10,10,10) = \toppre = \max \fix(T)$. 
After initializing subproblems at $(9,10,10)$, $(10,9,10)$ and $(10,10,9)$, suppose that we find $S_1(9,10,10) = (9,8,10)$, $S_2(10,9,10) = (10,9,7)$, and $S_3(10,10,9) = (9,10,9)$. 
As argued above, this shows that $\fix(\toppre_1) \peq (9,8,10) \peq (10,9,10) = \toppre_2$, so that $\fix(\toppre_1) \sub \fix(\toppre_2)$. 
Similarly, our calculations with $S_2$ and $S_3$ show that $\fix(\toppre_2) \sub \fix(\toppre_3)$ and $\fix(\toppre_3) \sub \fix(\toppre_1)$. 
Combining all these relations, apparently $\fix(\toppre_1) = \fix(\toppre_2) = \fix(\toppre_3) \peq (9,8,7)$, so the subproblems are equivalent.  
Thus, we can collapse all of these subproblems to a new subproblem started at $\pre_4 = (9,8,7)$ \emph{without losing any fixed points} of the original operator $T$. 

We have seen that collisions between subproblem cones give information relations of the form $\fix(\toppre_i) \sub \fix(\toppre_j)$ regarding the fixed points of $T$.
Consider a digraph that tracks these collisions.
Then this digraph has the form shown below, where subproblems correspond to vertices, and there is an edge $v_i \to v_j$ only if problem $i$ ``collides'' with problem $j$ (made precise below). 
Since, in particular, the digraph has a directed edge $(v_i,v_j)$ only if $\fix(\pre_i) \sub \fix(\pre_j)$, we see that the equivalence of subproblems is reflected in the connectedness of the graph. 

/

////// INSERT FIGURE - collision digraph and problem tree //////

/


Suppose now that $\toppre$ is found at some intermediate step in the algorithm, and consider a distinct subproblem near $\toppre$ started at $\pre = (11,11,11)$ with associated isotone operator $S$.
We begin iterating and find that $S \pre = (10,10,8)$, which shows $\fix(\pre) \sub \fix(\toppre_3)$ 
Then a \emph{single iteration} of $S$ has shown us that $\fix(\pre) \sub \fix(\toppre_3) \peq (9,8,7)$, using the relations above. 
Of course, the same conclusion would hold if we found that  $S \pre \peq \toppre_i$ for any $i$. 

We know that $\fix(\toppre_1) = \fix(\toppre_2) = \fix(\toppre_3) \peq \pre_4$. 
We can track these subproblem relations efficiently with a collection of rooted trees as follows: let $1,2,3$ be children of $4$, which we denote by $c(4) = \{1,2,3\}$.
Using the notation in the previous section, we have the root-vertex relation $r(i) = 4$ for $i = 1,2,3$. 
By an abuse of notation, we will similarly write e.g. $r(\toppre_1) = \pre_4$. 
By our construction, we have $\fix(\toppre_i) \sub \fix(r(\toppre_i))$ for all $i$. 
Also, $\fix(\toppre_i) = \fix(\toppre_j)$ for all $i,j$ children of the same node. 

Suppose we maintain a collection of trees for which nodes, roots, and children are all related in this way. 
Consider a solved subproblem $\lag \pre_i \rag$ with $r(\pre_i) = \pre_k$; that is, $k$ is the root of $i$ in our problem tree.
Then if at some step of the algorithm we obtain a guarantee that $\fix(\pre_j) \sub \fix(\pre_i)$ for an active subproblem $\lag \pre_j \rag$, the relations built into our problem tree imply that, in fact, $\fix(\pre_j) \sub \fix(\pre_k)$, potentially \emph{skipping a large region of lattice space} that we would otherwise have to search for elements of $\fix(T)$. 
This dynamic gives our algorithm a ``Chutes and Ladders'' effect. 
In particular, we make efficient use of both the geometric information shared between active problems as well as the information gained during previous steps of the algorithm. 

\subsubsection{Algorithm - Description and Intuition} 

In this section, we introduce notation and give a brief, intuitive explanation of the full algorithm, building on the examples above.
Pseudocode and correctness proofs for the full algorithm follow. 
First, we discuss indexing. 

\emph{Labels for Preallocations, Vertices, and Operators}:
During the algorithm, we will build a collection of graphs.
For any of these graphs, each vertex of the graph will correspond to a preallocation $\pre \in \prealloc$.
We thus require a way to denote correspondences between vertices and preallocations. 
We thus fix an index set $\mathcal{I}$, which uniquely labels every preallocation in $\prealloc$. 
We will denote corresponding vertices and preallocations with the same subindex, for instance, $\pre_i \sim v_i \sim w_i$, where $i \in \mathcal{I}$. 
We will also let $v(\pre)$ and $w(\pre)$ denote the vertices corresponding to a certain preallocation. 
Thus, for instance, $v(\pre_i) = v_i$.
For clarity, vertices of a digraph $G$ are always denoted by $v$, while vertices of a forest $F$ are always denoted by $w$.
Similarly, the operator for a subproblem $\pre_i$ will be denoted by $T_i$.
The algorithm consists of two alternating stages. 

\emph{Stage 1 - Information Acquisiton}: In the first stage, we consider a set of preallocations $\acto$, each of which corresponds to the maximal point of an active subproblem. 
As above, each problem $\lag \pre_i \rag$ is associated with an isotone operator $S_i = T_i^2$.
In this stage, we iteratively apply $S_i$ to each $\pre_i \in \acto$.
We define two more collections of preallocations.
Let $\acta$ be the collection of preallocations of the form $\{S_i^k \pre_i: \pre_i \in \acto\}$, which contains the current value of the Tarski sequence for each subproblem $\pre_i \in \acto$ that we are still working on.
Let $Q$ be a set of preallocations $\acto \sub Q$, which we think of as the past and current subproblems that still contain useful information.  
We initialize $\pre \leftarrow \pre_i$ and successively iterate $\pre \leftarrow S_i \pre$ until one of the following occurs \emph{stopping conditions} occurs
\begin{enumerate}
\item $\pre \peq \ul{\pre} = \min \fix(T^2)$ 
\item $T \pre = \pre$
\item $S_i \pre = \pre$
\item $\pre \peq \pre'$ for some $\pre' \in Q$ with $\pre_i \not \peq \pre'$ 
\end{enumerate} 

In case (1), $\fix(\pre_i) \peq \min \fix(T^2)$, so $\fix(\pre_i) = \emptyset$, and we may stop iteration. 
In case (2), the Tarski sequence for $\pre_i$ stops, so we stop the problem $\lag \pre_i \rag$ and form new subproblems $\lag \pre - e_j \rag$ for $j = 1 \hdots m$, each of which we add to $\acto$.  
In case (3), $\pre = S_i^k \pre_i$ is fixed by $T$, so by lemma xxxx, $\fix(T) \cap \{\pre' \pe \pre \} = \emptyset$.
That is, we can stop searching this problem.
In case (4), we have learned that $\fix(\pre_i) \sub \fix(\pre')$ for some other problem $\pre'$, so we stop work on $\preo_i$ and wait for the next stage of the algorithm, where we will combine the guarantees on $\fix(\pre')$ with those on $\fix(\pre_i)$.

During stage 1, we obtain intermediate states $\pre$ as above.
Any such state is of the form $\pre = S_j^k \pre_j$ for some $\pre_j \in \acto$.
When the context is clear, we denote this $\pre_j$ by $\preo$. 
Similarly, for any state $\pre_i \in \acto$, we let $\pref_i$ denote the final value of the problem $\lag \pre_i \rag$'s Tarski sequence before it stops, as above. 

\emph{Stage 2 - Information Combination}: In this stage, we use efficient digraph algorithms to combine information from related problems.
First, we define a graph structure relates subproblems solved during the algorithm
\begin{defn}
A set of rooted trees $\forest$ is said to be \emph{problem forest} for a problem collection $\act$ if
\end{defn}

\begin{enumerate}
\item If $\pre_k \in Q$, then $w_k \in \forest$. \label{def:forest1}
\item If there exists a $w_k \in F$ such that $w_i, w_j \in c(w_k)$, then $\fix(\pre_i) = \fix(\pre_j)$. \label{def:forest2}
\item If $w_i = p(w_j)$, then $\fix(\pre_j) = \fix(\pre_i)$ and $\pre_i \peq \pre_j$ \label{def:forest3} 
\end{itemize} 

Thus, siblings in a problem forest share fixed points of $T$, and the fixed points of a child are the same as the fixed points of its parent. 
For now, we assume the existence of such a forest for the problems considered below (proof in section~\ref{fullalgo}). 

For each $\pre_i \in \acto$, form a digraph $G = (V,E)$ by letting $(v_i,v_k) \in E$ if and only if there exists $\pre_j \in Q$ such that (i) $\pref_i \peq \pre_j$, (ii) $\pre_i \not \peq \pre_j$, and (iii) $r(w_j) = w_k$.
That is, by the correspondence between vertices and preallocations, during Stage 1 $\pre_i$ ``collided'' with some vertex $w_j$ in the problem forest which has $w_k$ as its root.  

Next, we form the strong components\footnote{Tarjan's algorithm for computing strong components of an arbitrary digraph $G$ runs in $\mathcal{O}(|V| + |E|)$ time.}
of the digraph $G$, denoted by $\strongc = \{\strongcomp_j\}_{j\in \mathcal{J}}$, where $\mathcal{J}$ is some index set. 
For $j \in \mathcal{J}$, pick $v \in \strongcomp_j$ and compute the reachable set of $v$, by which we define $\reach_j$, the reachable set of component $j$. 
Since strong components are strongly connected, this definition is independent of the choice of $v$. 

By our construction of the collision digraph and inductive assumption on the forest $\forest$, we have $\fix(\pre_i) \sub \fix(\pre_j)$ for any $i,j$ such that $v_k \in \strongcomp_k$ and $v_j \in \reach_k$, that is, whenever $v_j$ is in the reachable set of the strong component containing $v_i$. 

Let $Q' = \emptyset$ and, for each strong component $\strongcomp_j \in \strongc$, add $\pre = \min\limits_{v_k \in \reach_j} \pre_k$ to $Q'$. 
By the guarantees above, we can combine all previous problems into a new active set by letting $\acto = Q'$ without missing any points of $\fix(T)$.  

Note that, by changing $\acto$ in this way, we may have caused more cone collisions.
We can combine the geometric information from these collisions as above to weakly improve the progress of the algorithm.
Thus, we run stage 2 iteratively until there are no more cone collisions. 
This process terminates, as discussed in the appendix.  
After stage 2 terminates, we return to stage 1 with the new active set $\acto$.

In general, we would like to spend more time in stage 2, since stage 2 improves the progress of the search by using fast digraph algorithms, as opposed to the often costly process (how costly xxxx) of computing the operator $T$, equivalent to checking whether an allocation is strict core.  

\subsubsection{Full Algorithm and Correctness} \label{fullalgo}
In this section, we state the full algorithm and provide correctness proofs.
First we define the main subroutines, which implement each of the stages discussed above. 
Throughout, we maintain global variables $\forest$, a problem forest, and $\act$, a collection of preallocations corresponding to both active and inactive problem cones. 

To initialize the algorithm, set $\fixfind = \emptyset$ and $F = \emptyset$. 
Let $\pre^*$ and $\pre_*$ denote the maximal and minimal allocations in $\prealloc$. Apply $T^2$ iteratively to each to find $\toppre = \max \fix(T^2)$ and $\ul{\pre} = \min \fix(T^2)$, respectively.
If $T\toppre = \toppre$ set $\fixfind = \{\toppre\}$ and terminate.
Similarly, if $T\ul{\pre} = \ul{\pre}$, then set $\fixfind = \{\ul{\pre}\}$ and terminate. 
Otherwise, for $i = 1 \hdots m$ add $\toppre - e_i$ to $\act$ and $\acto$ and add $w(\pre - e_i)$ to $F$ as an isolated root. 
 
\begin{algorithm}
\caption{InformationAcquire - Subroutine for Stage 1}     
\label{infac}
\begin{algorithmic}[1]
\REQUIRE $\acto$
    \STATE Set $\acta \leftarrow \acto$ \label{alg1:add1}
    \STATE Set $\fixtemp \leftarrow \emptyset$ and $\actc \leftarrow \emptyset$ and $\coll \leftarrow \emptyset$
    \FOR{$\pre_i \in \acto$} \label{alg1:for1} 
        \IF[Remove redundant problems]{$\exists \pre_j \in \acto$ s.t. $\pre_i \pe \pre_j$ \OR $\pre_i \peq \min \fix(T^2)$}
            \STATE Remove $\pre_j$ from $\acta$, $\acto$, and $\act$ \label{alg1:stop0} \label{alg1:redundant}
        \ENDIF
    \ENDFOR
    \WHILE{$\acta \not = \emptyset$} \label{alg1:while1}
        \FOR{$\pre_i \in \acta$} \label{alg1:for2}
        \STATE $\pre \leftarrow \pre_i$
            \WHILE{$\pre_i \in \acta$} 
                \STATE Compute $\pre' = T_i^2 \pre$
                \IF[Stopping conditions 1 and 3]{$\pre' = \pre$ \OR $\pre' \peq \bopre$}
                    \STATE Remove $\pre_i$ from $\acta$ \label{alg1:stop1}
                    \STATE Set $\pref_i \leftarrow \pre'$; Add $w(\pref_i)$ to $F$ with $c(w(\pref_i)) = w(\pre_i)$ \label{alg1:tree1}
                    \IF[Stopping condition 2]{$T\pre = \pre$} 
                        \STATE Add $\pre$ to $\fixtemp$
                    \ELSIF{$\pre' = \pre$}
                        \FOR{$i = 1$ to $m$}
                            \IF[Add new problems]{there is no $\pre'' \in \acto$ with $\pre - e_i \peq \pre''$ } \label{alg1:redundant}
                                \STATE Add $\pre - e_i$ to $\acto$, $\acta$, and $\act$ \label{alg1:addq}
                                \STATE Add $w(\pre - e_i)$ to $\forest$ as an isolated root 
                            \ENDIF
                        \ENDFOR
                    \ENDIF
                \ELSE
                    \FOR{$\pre_j \in Q$}
                        \IF[Stopping condition 4]{$\pre' \peq \pre_j$ \AND $\pre_i \not \peq \pre_j$} \label{alg1:impact}
                            \STATE Remove $\pre_i$ from $\acta$ \label{alg1:stop2}
                            \STATE Set $\pref_i \leftarrow \pre'$; Add $w(\pref_i)$ to $F$ with $c(w(\pref_i)) = w(\pre_i)$  \label{alg1:tree2}
                            \STATE Add $\pre_i$ to $\actc$; Add $j$ to $\coll(i)$ \COMMENT{Build collision indices} \label{alg1:coll}
                        \ENDIF
                    \ENDFOR
                \ENDWHILE
            \ENDIF
        \ENDFOR
    \ENDWHILE
    \RETURN{$(\actc, \coll, \fixtemp)$} 
\end{algorithmic}
\end{algorithm}

Next, consider the information combination algorithm. 

\begin{algorithm}
\caption{InformationCombine - Subroutine for Stage 2}  
\label{infcomb}
\begin{algorithmic}[1]
\REQUIRE $\acto$, $\coll$
    \STATE Set $V = E = \emptyset$
    \STATE Set $\actt = \emptyset$
    \FOR[Build collision digraph]{$\pre_i \in \acto$}
        \STATE Add $i$ to $\coll(i)$ \label{alg2:self_int}
        \STATE Add $v_i$ to $V$ \label{alg2:selfref}
        \FOR{$j \in \coll(i)$}
            \STATE Compute $w_k = r(w_j)$ \label{alg2:rootbuild}
            \STATE Add $v_k$ to $V$
            \STATE Add $(v_i,v_k)$ to $E$ %no multi-edges - only add once
        \ENDFOR
    \ENDFOR
    \STATE Set  $G = (V,E)$
    \STATE Compute strong components $\strongc$  of $G$
    \STATE Remove $\{v_k\}\in \strongc$ for $\pre_k \not \in \acto$, leaving components $\{S_j\}_{j \in \mathcal{J}}$ \label{alg2:cstrong}
    \FOR[Use $G$ to combine information]{$j \in \mathcal{J}$} \label{alg2:for2}
        \STATE Compute reachable component $\reach_j$ 
        \STATE Compute indices $K_j = \{k \in \mathcal{I}: v_k \in \strongcomp_j\}$
        \STATE Set $\pre = \min_{k \in K_j} \pre_k$ and add to $\actt$
        \STATE Add $w(\pre)$ to $F$ with $c(w(\pre)) = \{r(w_k): v_k \in \strongcomp_j\}$ \label{alg2:forest}
    \ENDFOR
    \STATE Set $\coll(i) = \emptyset$ for $i \in \mathcal{I}$
    \FOR{$\pre_i \in \actt$} 
        \FOR{$\pre_k \in Q$}
            \IF{$\exists w_j \in c(w_i)$ s.t. $\pre_i \peq \pre_k$ \AND $\pre_j \not \peq \pre_k$} \label{alg2:collide}
                \STATE Add $j$ to $\coll(i)$ \label{alg2:coll2} 
            \ENDIF
        \ENDFOR
    \ENDFOR
    \STATE Set $\acto \leftarrow \actt$ 
    \STATE Set $\act \leftarrow \act \cup \actt$ \label{alg2:addq}
\RETURN{$(\acto,\coll)$} 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{FindStrictCore} 
\label{mainalg}
\begin{algorithmic}[1]
\REQUIRE $\acto$
\ENSURE $\fix(T^2)$
    \STATE Initialize as above
    \STATE Set $\fixfind = \emptyset$
    \WHILE{$\acto \not = \emptyset$} \label{alg3:while1}
        \STATE $(\acto, \coll, \fixtemp) \leftarrow InformationAcquire(\acto)$ 
        \STATE $\fixfind = \fixfind \cup \fixtemp$
        \WHILE{$\coll \not = \emptyset$} \label{alg3:while2}
            \STATE $(\acto,\coll) \leftarrow InformationCombine(\acto,\coll)$ 
        \ENDWHILE
    \ENDWHILE
\RETURN $\fixfind$
\end{algorithmic}
\end{algorithm}

First, we handle termination analysis.
\begin{lemma}
FindStrictCore terminates in finite time
\end{lemma}
\prf
It suffices to show that each while loop in the algorithm terminates in a finite number of iterations. 
For any set $E \sub \prealloc$, we define $\disto(E) = \max_{\pre \in E} \|\pre\|_1$, where the set of preallocations $\prealloc$ is identified with a grid as above\footnote{For $\x \in \mr^m$, $\|x\|_1 = \sum_{i = 1}^m |x_i|$ is the standard 1-norm .}.
Similarly, we define $\distt(E) = \sum_{\pre \in E} \|\pre\|_1$.
Note that by finiteness, both $\disto(\cdot)$ and $\distt(\cdot)$ take finitely many values on subsets of $\prealloc$.
First we show that, for any input $\acto \sub \prealloc$, subroutine \ref{infac} terminates in finitely many iterations.

Consider the sequence $\actak$ formed by distinct states of $\acta$ during the execution of subroutine~\ref{infac}.
Note that if for any $k\geq 0$ we have $\acta^{k+1} = \actak \setminus \{\pre\}$ for some $\pre \in \prealloc$, then $\disto(\acta^{k+1}) \leq \disto(\actak)$.
By inspection, all changes to $\acta$ except for the change on line~\ref{alg1:addq}, where we add new problems, are of this form.
For line~\ref{alg1:addq}, note, for any preallocation $\pre$, we have $\|\pre\|_1 > \max\limits_{i = 1,\hdots,m} \|\pre - e_i\|$, so, in fact, if $\actak \to \acta^{k+1}$ on line~\ref{alg1:addq} then $\disto(\acta^{k+1}) \leq \disto(\actak)$. 
This shows that $\disto(\actak)$ is a monotonically decreasing sequence bounded below by $0$. 
Since $\disto(\actak)$ takes at most finitely values, this sequence of real numbers converges in finite time.

Note that all of the inequalities above are \emph{strict} when $\pre^* = \argmax_{\pre' \in \acta} \|\pre'\|_1$. 
Fix $\acta^{0}$ at the beginning of the for loop on line~\ref{alg1:for2}. 
Then $\pre^* \in \acta^{0}$, so, for at least one iteration of the loop~\ref{alg1:for2}, $\disto(\actak)$ decreases strictly.
Since $\disto(\actak)$ converges in finite time, the for loop at line~\ref{alg1:for2} is executed at most finitely many times. 
Then the while loop at line~\ref{alg1:while1} is also executed finitely many times, so this subroutine terminates.

Since all inputs are finite, subroutine~\ref{infcomb} clearly terminates in finite time.

Next, we show that the information combination step in FindStrictCore terminates (the 2nd while loop of algorithm~\ref{mainalg} terminates).
Consider the distinct states of $\acto$, which form a sequence $\actok$, during the execution of this loop.
We will show that the sequence of positive real numbers $\distt(\actok)$ is monotonically decreasing.

Consider a single execution of the subroutine InformationCombine with input $\acto$.
Let $\strongcomp \in \strongc$ be a strong component of the collision digraph $G = (V,E)$ left in the collection $\{\strongcomp_j\}_{j \in \mathcal{J}}$ after removing irrelevant singletons in line~\ref{alg2:cstrong}.
By construction, the out-degree of a vertex $v_i \in V$ is $0$ if $\pre_i \not \in \acto$
By definition of a strong component, then, each vertex $v_i$ \emph{not} associated with a preallocation $\pre_i \in \acto$ generates a singleton strong component $\{v_i\}$, which is removed at line~\ref{alg2:cstrong}
Then $\{v_i: v_i \in \strongcomp_j; j \in \mathcal{J}\} \sub \{v_i: \pre_i \sub \acto\}$. 
Therefore, by disjointness of strong components $\{\strongcomp_j\}_{j \in \mathcal{J}}$, we can define a surjection $p: \acto \to \actt$ by setting $p(\pre_i) = \min_{k \in K_j} \pre_k \in \actt$. 
This also shows, by construction, that for any $\pre_j \in \actt$, we have $p\inv(\{\pre_j\}) \supseteq \{\pre_k: w_k \in c(w_j)\}$. 
Note that, since we add $i$ to $\coll(i)$ at line~\ref{alg2:self_int}, if $\pre_i \in \strongcomp_j$ and $r(w_i) = w_{\ell}$, then $v_i \to v_{\ell}$; in particular, $v_{\ell} \in \reach_j$, the reachable set of strong component $j$.
Lemma~\ref{lemma:key} shows that if $\pre_k = p(\pre_j)$, then $\pre_k \peq \pre_j$. By iterating, it follows $\pre_{\ell} \peq \pre_i$. %break into lemma statement? 
Then apparently $p(\pre_i) = \min_{k \in K_j} \pre_k \peq \pre_i$ for any such $i$. 

Using the map $p$ above, we can express $\acto$ as a disjoint union of the form $\acto = \cup_{\pre \in \actt} p\inv(\pre)$.
Since $p$ is a surjection, we can calculate $\disto(\actt) = \max_{\pre_i \in \actt} \|\pre_i\|_1 \leq \max_{\pre_i \in \actt} \disto(p\inv(\pre_i)) = \disto(\acto)$. 
Similarly, since $\distt(\cdot)$ is an additive set function, we compute $\distt(\actt) = \sum_{\pre \in \actt} \distt(\{\pre\}) \leq \sum_{\pre \in \actt} \distt(p\inv(\pre)) = \distt(\acto)$ by surjection and the discussion above.
Since $\actt$ is returned from the subroutine as $\acto^{k + 1}$ given input $\actok$, we have shown $\disto(\acto^{k+1}) \leq \disto(\actok)$ and $\distt(\acto^{k+1}) \leq \distt(\actok)$ for any $k$. 

In particular, the sequence $\distt(\actok)$ constructed above is monotonically decreasing (and bounded below by 0), so it converges at a finite value of $k$, again using finiteness of $\prealloc$. 
Suppose that for $k \geq 0$ with $\actok$ the input to the subroutine InformationCombine, we have $\distt(\actok) = \distt(\acto^{k+1})$.
By the previous discussion, $\distt(p\inv(\pre_j)) \geq \distt(\{\pre_j\})$ for any $\pre_j \in \actt$, so each of these weak inequalities must actually be equality. 
Since, in addition, $p(\pre_i) \peq \pre_i$ for $\pre_i \in \actok$, each of these inequalities under the partial order $\peq$ must also be an equality, with $p\inv(\pre_i) = \{\pre_i\}$ for any $\pre_i \in \actt$.
Then $\actok = \actt = \acto^{k+1}$. 
This argument shows that the \emph{sequence of sets} $\actok$ actually converges in finite time as well. 

Suppose, then, that $\actok = \acto^{k+1}$ for some $k$ and consider a single execution of InformationCombine. 
Above, we showed that $p\inv(\pre_i) = \pre_i$ for any $\pre_i \in \actt$.
Then, as noted above, we have $\{i\} = \{j: \pre_j \in p\inv(\pre_i)\} \supseteq \{j: w_j \in c(w_i)\}$, so the expression defining a collision in line~\ref{alg2:collide} must be false for all $\pre_i \in \actt$. Then the subroutine must return $I = \emptyset$. 
This shows that the information combination stage of the algorithm terminates.
That is, every sequence of executions of subroutine~\ref{infcomb} is finite.

We have shown that the sequence $\disto(\actok)$ is monotonically decreasing during the information combination stage (the second while loop in FindStrictCore) as well as during information acquisition in subroutine~\ref{infac}
Then the sequence $\disto(\actok)$ formed by distinct states of $\acto$ at any time (within the outer while loop) during FindStrictCore is also monotonically decreasing.  
We complete the termination proof by showing that $\disto(\acto)$ decreases strictly at \emph{least once} during an iteration of the outer while loop of FindStrictCore. Consider evaluating subroutine~\ref{infac} with input $\acto$. 

\emph{Case 1}: The collection $\actc$ returned by the subroutine is empty. Note that, by inspection, $\acto \not = \emptyset$. 
Then apparently $0 = \disto(\acto^{k+1}) = \disto(\actc) < \dist(\actok)$. 

\emph{Case 2}: Suppose $\actc \not = \emptyset$ and consider $\pre_i \in \actc$.
Since $\pre_i$ was added to $\actc$, there must exist $k \geq 0$ such that $(T_i^2)^k \pre_i \not = (T_i^2)^{k+1} \pre_i$ (line~\ref{alg1:coll} of subroutine~\ref{infac}).  
In particular, by monotonicity of the Tarski sequence for $\lag \pre_i \rag$, we must have $\pre_i^f \pe \pre_i$ \emph{strictly}.
Note that, for all such $\pre_i \in \actc$, we add $w(\pre_i^f) = p(w_i)$ (at line~\ref{alg1:tree2}). Then $r(w_i) = r(w(\pre_i^f))$. Denote this root by $r(w(\pre_i))= w_k$. 
The algorithm returns $\actc = \acto'$, which is an input for subroutine~\ref{infcomb}.
In subroutine~\ref{infcomb}, we form strong components of the collision digraph $G = (V,E)$. 
Suppose that $W_j$ is the strong component containing $v_i$.
We add $i$ to $\coll(i)$ before forming the edges of $G$ (lines~\ref{alg2:selfref} and~\ref{alg2:rootbuild}), so we must have $(v_i,v_k) \in E$.
Then $v_k \in \reach_j$, the reachable set of strong component $\strongcomp_j$. 
Therefore, $\pre' = \min_{\ell \in K_j} \pre_{\ell} \peq \pre_k \peq \pre_i^f \pe \pre_i$, where the second inequality follows because $w(\pre_i^f)$ is a descendant of $w_k$. 
Using the map $p:\acto' \to \actt$ above, we have $p(\pre_i) = \pre'$.
We calculate as before $\disto(\actt) = \max_{\pre_j \in \actt} \|\pre_j\|_1 < \max_{\pre_j \in \actt} \disto(p\inv(\pre_j)) = \disto(\acto)$, where the inequality is strict by the work above, noting that $\pre_i \in p\inv(\{\pre'\})$. 
Since $\acto \leftarrow \actt$ when this subroutine returns, we have shown that, in both cases above, the sequence $\disto(\actok)$ decreases \emph{strictly} during a single execution of the outer while loop in FindStrictCore. 

Above, we argued that $\disto(\actok)$ converges in finite time. 
Therefore, the outer while loop of FindStrictCore executes only finitely many times, so the algorithm terminates. 
\eprf 

Our proof of correctness will make use of the following key lemma, which shows that we can track the relations between subsets of $\fix(T)$ with the graph $F$ constructed during the algorithm.

\begin{lemma} \label{lemma:key}
At any point during the execution of FindStrictCore 
\begin{enumerate}
\item $\forest$ is a problem forest \label{convlemma:forest}
\item If $k \in \coll(i)$, where $r(w_i) = w_j$, then $\fix(\pre_j) \sub \fix(\pre_k)$ \label{convlemma:coll}
\end{enumerate}
\end{lemma}

First, we give definitions and some useful auxillary lemmas. 
For a forest (a collection of rooted trees) define the \emph{root set} $\roott(\forest) = \{r(w): w\in F\}$. 
That is, $\roott(\forest)$ consists of all roots of trees in the forest.
For clarity, if $t$ is a rooted tree in $\forest$, we will let $r(t)$ denote the (unique) root node of this tree.
The depth of a vertex $w$ in a forest $\forest$ is defined as its depth in the unique rooted tree containing it, denoted by $\depth(w, \forest)$

Next, we develop some needed graph-theoretic results for problem forests. 

\begin{lemma} \label{lemma:forest}
Let $\forest$ be a rooted forest. 
Suppose that $\forest'$ is obtained from $\forest$ in one of the following ways 
\begin{enumerate}
\item Adding an isolated vertex to $\forest$ \label{forest:solo}
\item Adding a vertex $w$ and directed edges $(w,w')$ such that $w' \in \roott(\forest)$. \label{forest:child}
\end{enumerate}
Then the following are true  
\begin{enumerate}
\item[(i)] $\forest'$ is also a rooted forest. 
\item[(ii)] The reachable set $\reach(w,\forest)$ is unchanged by the modification $\forest \to \forest'$. 
\item[(iii)] The depth $\depth(w,\forest)$ of a vertex $w \in \forest$ weakly increases. 
\end{enumerate}


\end{lemma}

\prf
Note that a graph $G = (V,E)$ is a tree\footnote{Gross and Yellen xxxx} if and only if $G$ is connected and $|E| = |V| - 1$. 
Consider the forest $\forest$ and let $\{t_i\}_{i \in I}$ be its constituent rooted trees. 
That is, $\{t_i\}_{i \in I}$ is the collection of maximal connected components of $\forest$.
For case (1), no edges were added, so the new collection of maximal connected components is now $\{t_i\}_{i \in I} \cup \{w\}$. 
Each $t_i$ is still a rooted tree by assumption, and $\{w\}$ is also trivially a rooted tree with root $w$. 

For case (2) above, let $w$ be the added vertex and $J$ the subset of tree indices such that, for $i \in J$, edge $(w,r(t_i))$ is added to $\forest$.
Let $i \in J$. 
Since each such $t_i$ is connected and connectedness is transitive, then $t' = \cup_{i \in J} t_i$ is a (weak) connected component of $\forest'$. 
Then apparently the connected components of $\forest'$ are $\{t_i\}_{i \not \in J}$ and $\{t'\}$ . 
It suffices to show that $t'$ is a rooted tree. 
Let $V(t)$ and $E(t)$ denote the vertices and edges, respectively, of a tree $t$. 
By construction, $|V(t')| = 1 + \sum_{i \in J} |V(t_i)|$ and $|E(t')| = |J| + \sum_{i \in J} |E(t_i)| = |J| + \sum_{i \in J} (|V(t_i)| - 1) = \sum_{i \in J} |V(t_i)|$. 
Then $|V(t')| = |E(t')| + 1$, so, by the criterion above, $t'$ is a tree.

We show that $t'$ is rooted with root $w$.
Consider $w' \in V(t') \setminus \{w\}$. 
Then $w' \in V(t_i)$ for some $i \in J$. 
There exists a directed path $w \to \roott(t_i)$, and a directed path $\roott(t_i) \to w'$ because $t_i$ is a rooted tree. 
Then there is a directed path $w \to w'$, which is unique because $t'$ has no cycles. 
Then $t'$ is a tree rooted at $w$, so $\forest'$ is a rooted forest.

Next, we prove the statement about reachable sets. 
Case~(\ref{forest:solo}) above is clear, since no edges are added.  
For case~(\ref{forest:child}), let $w \in t$ for some tree in $\forest$. 
If there is a new path from $w \to w'$ such that $w' \not \in \reach(w, \forest)$, it uses one of the added edges $(w_1,w_2)$. 
Then, apparently, there exists a path $w \to w_1$ in $\forest'$. 
But, as shown above, $w_1$ is a root in the new forest $\forest'$, so there exists no directed edge of the form $(x,w_1)$. 
This is a contradiction, so it must be that $\reach(w,\forest) = \reach(w,\forest')$.

The final statement concerning vertex depth is trivial. 

\eprf

Next, we give a simple observation, showing how problem forests can be used to track fixed point relations. 

\begin{lemma} \label{lemma:forest2}
Let $\forest$ be a problem forest for $\act$, with $\pre_i, \pre_j \in \act$.
Let $t$ be a rooted tree in $\forest$, and suppose that $r(w_i) = w_j$. 
Then $\fix(\pre_i) = \fix(\pre_j)$. 
\end{lemma}

\prf 
By assumption, $t$ is a rooted tree, so there is a unique directed path of the form $w_j = w^0 \to w^1 \to \cdots \to w^n = w_i$ from the root to $w_i$. 
Then, for each vertex in the path is related by $p(w^k) = w^{k - 1}$ for $1 \leq k \leq n$. 
Then, since $\forest$ is a problem forest by assumption, we get a chain of inclusions $\fix(\pre_{k-1}) = \fix(\pre_k)$ for $1 \leq k \leq n$, showing that, as claimed, $\fix(\pre_i) = \fix(\pre_j)$. 
\eprf 

First, we show an auxillary lemma about the relationship between $\forest$ and the problem sets returned from each subroutine 

\begin{lemma} \label{lemma:roots}
Any distinct problem $\pre \in \prealloc$, is processed (the Tarski sequence for $\lag \pre \rag$ is evaluated) at most once.
Moreover, the following statements are true: 
\begin{enumerate}
\item If $\forest$ is a problem forest when subroutine~\ref{infac} is initialized, then the set $\acto$ returned from the subroutine satisfies $\pre_i \not = \pre_j \in \acto \implies r(w_i) \not = r(w_j)$. \label{lemma:roots:ac}
\item If $\acto$ is returned from subroutine~\ref{infcomb}, then $w(\pre) \in \roott(\forest)$ for each $\pre \in \acto$. \label{lemma:roots:comb}
\end{enumerate}
\end{lemma}
\prf 

We start with the first statement of the lemma.  
If $\pre_i$ is processed more than once, then $\pre_i$ is added to $\acta$ more than once during the algorithm . 
Since $\acta \sub \acto$, and elements are \emph{only added} (never removed) to $\acto$ after processing begins at line~\ref{alg1:while1}, then $\pre_i \in \acto$ when it is added to $\acta$ the second time. 
But we check for redundancy before we add a program in line~\ref{alg1:addq}, so this is impossible. 
Then any distinct preallocation is processed at most once. 

For the other lemma statements, we work by induction on $\ell$, the total number of evaluations of subroutines~\ref{infac} and \ref{infcomb}. 
For $\ell = 0$ the statements are trivially true. 
Then assume that (1) and (2) above hold up to $\ell = n \geq 0$. 
There are several cases. 
Throughout, let $(\acto',\forest')$ and $(\acto'',\forest'')$ denote the states of $(\acto,\forest)$ at the beginning and end, respectively, of the subroutine that returns at $\ell = n + 1$.  

\emph{Case 1}: Subroutine 1 returns at $\ell = n + 1$. 
Note that subroutine~\ref{infac} is called either directly after initialization or after evaluation of subroutine~\ref{infcomb}. 
Note that $\forest''$ is obtained from $\forest'$ by adding vertices and edges as in lemma~\ref{lemma:forest}. 
Then $\forest''$ is also a rooted forest by the lemma.
If the subroutine executes following initialization of FindStrictCore, clearly $w(\pre) \in \roott(\forest')$ for $\pre \in \acto'$ by construction.
Otherwise, this statement holds by the inductive hypothesis and (\ref{lemma:roots:comb}) above. 
Note that each $\pre$ added to $\acto$ during this subroutine is initialized as an isolated root. 
Then we have shown that, any time before $\pre$ is processed (the Tarski sequence for $\pre \in \acta$ is evaluated), $w(\pre) \in \roott(\forest)$. 
In particular, $\depth(w_i, \forest) = 0$ when $\pre_i$ is added to the $\acto$ during the subroutine. 

Suppose that for $\pre_i \not = \pre_j \in \acto''$, $r(w_i) = r(w_j) = w$ in $\forest''$. 
Then there are directed paths $w \to w_i$ and $w \to w_j$. 
The directed path $w \to w_i$ can be represented as a sequence of vertices $w = w^1,w^2,\hdots, w^k = w_i$ and similarly for $w_j$. 
Clearly, there is at leat one vertex in the path $w \to w_j$ not in $w \to w_i$. 
Let $k'$ denote the minimal index of such a vertex in either of the paths. 
By minimality, the paths are identical up to $w_{k' - 1}$. 

In subcase 1, one of the paths, without loss the path $w \to w_i$, ends at $w_{k'-1}$. 
Then there is a directed path $w_i \to w_j$.
But $\depth(w_j, \forest) = 0$ when $\pre_j$ is added to $\acto$ as above, so all edges on this path must have been added during the subroutine, with $\pre_i$ added after $\pre_j$, since in-edges are only added to problems already in $\acto$. %more formal 
By the monotonicity of the Tarski sequence for any problem, any edge $(w_1,w_2)$ added during the algorithm satisifies $\pre_1 \peq \pre_2$. 
In particular, $\pre_i \peq \pre_j$. 
But then $\pre_i$ is redundant given $\pre_j$, so it was not added to $\acto$ after $\pre_j$ (by line~\ref{alg1:redundant}). 
This is a contradiction, so there is no such path.

In subcase 2, both paths $w \to w_i$ and $w \to w_j$ continue beyond $w_{k'-1}$.
Then $w' \equiv w_{k' - 1}$ has out-degree $\geq 2$.
Only vertices of out-degree $1$ are ever added to $\forest$ during the subroutine, so we must have $w' \in \forest'$, the problem forest at the beginning of the subroutine.
Again, modifications to $\forest'$ during the subroutine satisfy the assumptions of lemma~\ref{lemma:forest}, so the reachable sets for $w'$ have $\reach(w',\forest') = \reach(w',\forest'')$.  
Then, in particular, $w_i \in \forest'$ and $\depth(w_i, \forest') \geq 1$
By lemma~\ref{lemma:forest} again, $\depth(w_i, \forest) \geq \depth(w_i, \forest') \geq 1$  for any state of the forest $\forest$ during the subroutine. 
Then $\pre_i$ is never in $\acto$ during the algorithm, since we showed that, for any $\pre_i \in \acto$, $w_i \in \roott(\forest)$ at some point during the algorithm. 
This completes the proof for this case.

\emph{Case 2}: Subroutine~\ref{infcomb} returns at $\ell = n +1$ after subroutine~\ref{infac} returns at $\ell = n$. 
We will show that $w(\pre) \in \roott(\forest'')$ for all $\pre$ in the set of returned problems $\actt$.
Let $G = (V,E)$ be the collision digraph formed during this subroutine.
By definition, the strong components $\{\strongcomp_j\}_{j \in \mathcal{J}}$ of $G$ retained after removing irrelevant singletons (at line~\ref{alg2:cstrong}) are disjoint. 
Then if $\strongcomp_j \not = \strongcomp_{j'}$ with $v_k \in \strongcomp_j$ and $v_{k'} \in \strongcomp_{j'}$ we must have $r(w_k) \not = r(w_{k'})$ by the inductive hypothesis and (\ref{lemma:roots:ac}) above, so $\{r(w_k): v_k \in \strongcomp_j\} \cap \{r(w_k): v_k \in \strongcomp_{\j'}\} = \emptyset$. 
Define $R(j,\forest) = \{r(w_k): v_k \in \strongcomp_j\}$, roots of $\forest$ generated by vertices in $\strongcomp_i$. 
At any point during the algorithm, call an index $j$ \emph{processed} if $\pre_j \in \actt$.

As we add problems $\pre$ to $\actt$ and vertices $w(\pre)$ to $\forest$, the problem forest $\forest$ changes, so root sets may change. 
However, working by induction the number of processed problems $m$, we can show that $R(\forest,j) = R(\forest',j)$ for any unprocessed index $j$ and forest state $\forest$ during this subroutine.
The base case $m = 0$ is trivial. 
Assume this is true up to $m = k$. 
Suppose we process index $j$ at $m = k+1$ and let $\unproc$ be the set of unprocessed indices, where $\forest_j$ is the state of the forest before processing index $j$. 
For $i \in \unproc$, $R(\forest_j,i) = R(\forest',i)$ for $i \in \unproc$ by induction.
In particular, $R(\forest_j,i)$ are pairwise disjoint for $i \in \unproc$.
Then adding a new vertex $w(\pre_j)$ as a root with $c(w(\pre_j)) = R(\forest',j)$ cannot change any of the other component root sets $R(\forest', \ell)$ for $\ell \in \unproc \setminus \{j\}$.
If $k$ is processed after $j$, this shows $R(\forest_k,\ell) = R(\forest',\ell)$ for $\ell \in \unproc \setminus \{j\}$, which completes the induction. 

In particular, for any $\pre \in \actt$, $w(\pre) \not \in \forest'$ at the beginning of the subroutine, so $w(\pre) \not \in R(\forest_j,j) = R(\forest',j)$ for any $j$. 
Then no edge of the form $(w', w(\pre))$ is ever added during this procedure. 
This shows that each added vertex $w(\pre)$ is \emph{still a root} in $\forest''$, when the subroutine returns, so we have shown (\ref{lemma:roots:comb}) above for this case. 

\emph{Case 3}: Subroutine~\ref{infcomb} returns at $\ell = n +1$ after subroutine~\ref{infcomb} returns at $\ell = n$.
By the inductive hypothesis for problems returned from subroutine~\ref{infcomb}, all inputs $\pre \in \acto'$ to subroutine~\ref{infcomb} at $\ell = n + 1$ have $w(\pre) \in \roott(\forest')$.
Then $R(\forest',j) = \{r(w_k): v_k \in S_j\} = \{w_k: v_k \in S_j\}$. 
Then the root sets are disjoint in this case as well, by disjointness of strong components.
The inductive argument from case 2 then establishes~(\ref{lemma:roots:comb}) above for this case as well. 
\eprf 
We are now ready for the proof of our key lemma.  %proof of key lemma 
\prf[Proof of Lemma~\ref{lemma:key}]
A straightforward induction on distinct states $(\forest,\act)_k$ of the pair $(\forest, \act)$ shows that condition~(\ref{def:forest1}) of the definition of a problem forest is always satisfied. 
Specifically, By inspection, any line in which we add $\pre$ to $\act$ (lines~\ref{alg1:addq} of subroutine~\ref{infac} and \ref{alg2:addq} of subroutine~\ref{infcomb}) is paired with a line where $w(\pre)$ is added to $\forest$. 

For the remaining statements, we work by induction on the sequence $\pair_k$ of distinct states of the pair $\pair$. 
For $k = 0$, note that we set $F$ to be a collection of isolated roots $w(\toppre - e_i)$ for $1 \leq i \leq m$, so clearly $F$ is a forest, and the other defining conditions are trivial. 
Initially, we set $\coll = \emptyset$, so item~\ref{convlemma:coll} above is trivially satisfied. 
Then assume by induction that the statements above hold for $\pair_k$ up to $k = n \geq 0$. 
By an abuse of notation, we will denote the components of $\pair_k$ as $\forest_k$ and $\coll_k$ when the meaning is clear.
Consider separately each case where the pair $\pair$ can change.

\emph{Case 1}: There is a transition $\pair_n \to \pair_{n +1}$ in subroutine ~\ref{infac} when stopping condition (1) or (2) is triggered, and $w(\pre_i^f)$ is added to $\forest$ (line~\ref{alg1:tree1}). 
Let $\forest'$ and $\acto'$ denote the state of $\forest$ and $\acto$ at the beginning of subroutine~\ref{infac}.
From lemma~\ref{lemma:roots}, we know that (i) any distinct problem $\pre \in \acto$ is processed (the Tarski sequence for $\lag \pre \rag$ evaluated).
Moreover, the lemma also shows that (ii) any $\pre \in \acto'$ has $w(\pre) \in \roott(\forest')$ and, from the proof, (iii) any $\pre$ added to $\acto$ during the subroutine has $w(\pre) \in \roott(\forest)$ before $\pre$ is processed. 
We will use these facts below. 

At state change of $\pair_n \to \pair_{n + 1}$, we set $\pre_i^f = \pre' = (T_i^2)^{\ell}$ for some $\ell \geq 0$.
By the Tarski argument from the main text, we have $\fix(T_i) \peq \max \fix(T_i^2) \peq (T_i^2)^{j}$ for any $j$. 
In particular, $\fix(T_i) \peq (T_i^2)^{\ell} = \pre'$.
Combining this with lemma~\ref{lemma:contain} above, $\fix(\pre_i) = \fix(T) \cap \{\pre'' \peq \pre_i\} \sub \fix(T_i) \peq \pre' = \pre_i^f$. 
By monotonicity of the Tarski sequence, we have $\pre_i^f \peq \pre_i$, so $\fix(\pre_i^f) \sub \fix(\pre_i)$, so $\fix(\pre_i) = \fix(\pre_i^f)$.
Since $w(\pre_i^f) = p(w_i)$ by definition, we have shown that $\fix(\pre_i) = \fix(p(\pre_i))$ for the only \emph{new} parent-child relation added to $\forest_n$. 
We add $c(w(\pre_i^f)) = \{\pre_i\}$ by definition, so there are no new sibling relations. 
Then by the inductive hypothesis, $\forest_{n+1}$ also satisfies condition~(\ref{def:forest2}) and (\ref{def:forest3}) defining a a problem forest.
By (iii) above, $\forest_n$ satisfies $w(\pre_i) = r(w(\pre_i))$, so the modification $\forest_n \to \forest_{n + 1}$ satisfies the conditions of lemma~\ref{lemma:forest}.
The, by the lemma, $\forest_{n + 1}$ is also a rooted forest. 
This completes the proof that $\forest_{n + 1}$ is a problem forest.

Finally, by mutual exclusion of stopping conditions and single-processing condition (i) above, we must have $\coll(i) = \emptyset$, so item~(\ref{convlemma:coll}) above holds trivially.

\emph{Case 2}: There is a transition $\pair_n \to \pair_{n +1}$ in subroutine~\ref{infac} during processing of $\pre_i$, where stopping condition (4) is triggered and $w(\pre_i^f)$ is added to $\forest$ (line~\ref{alg1:tree2}). 
By the exact same arguments as in case 1, $F_{n + 1}$ remains a problem forest after this modification. 
We need only check item~(\ref{convlemma:coll}) of the inductive hypothesis above.
$\coll_n(i) = \emptyset$, since $I = \emptyset$ at subroutine initialization, and each preallocation is processed at most once. %again, break this off as a lemma%
$\coll_n$ is modified to $\coll_{n + 1}$ when we find that $\pre_i^f \peq \pre_j \in \act$, at which point $j$ is added to $I_n(i)$, so $I_{n + 1}(i) = \{j\}$. 
By the note at the beginning of case 1, $w(\pre_i) \in \roott(\forest_n)$ since each preallocation is processed at most once, $w(\pre_i) \in \roott(\forest_n)$. 
Since we add $w(\pre_i^f) = p(w_i)$, we have $w(\pre_i^f) = r(w_i)$. 
Since $\pre_i^f \peq \pre_j$ we are done for index $i$. 

Now consider an index $\ell \not = i$, so that $\coll_n(\ell) = \coll_{n + 1}(\ell)$. 
Suppose that $k \in \coll(\ell)$. 
By the inductive hypothesis, for $\forest = \forest_n$, if $r(w_{\ell}) = w_j$, then $\fix(\pre_j) \sub \fix(\pre_k)$. 
If we still have $r(w_{\ell}) = w_j$, for $\forest = \forest_{n+1}$, we are done by induction. 
If not, then it must be that $r(w_{\ell}) = w(\pre_i^f)$. 
But $\ell \not = i$, so $w_{\ell}$ is a descendant of $w_i$ in $\forest_n$.
By construction, all elements $\pre \in \acto$ have $w(\pre) \in \roott(\forest')$, where $\forest'$ is a previous state of the problem forest, or $w(\pre)$ is initialized as an isolated root, so this descendant relation is impossible.
Then condition~(\ref{convlemma:coll}) holds and we are done. 

The case where we add $i$ to $\coll(i)$ in subroutine~\ref{infcomb} holds as an extension of work above, since by single processing we have $r(w_i) = w(\pre_i^f)$, and $\pre_i^f \peq \pre_i$ by the Tarski argument discussed previously. 

\emph{Case 3}: There is a transition $\pair_n \to \pair_{n + 1}$ by adding a collision index $j$ to $\coll_n(i)$ for some newly created problem $\pre_i \in \actt$ in line~\ref{alg2:coll2} of algorithm~\ref{infcomb}.
Then $\forest_n = \forest_{n + 1}$, so it suffices to check that condition~(\ref{convlemma:coll}) holds. 
Let $r(w_i) = w_k$, so, by induction, $\fix(\pre_i) = \fix(\pre_k)$. 
Now, by assumption, $\pre_i \peq \pre_j$, so $\fix(\pre_i) \sub \fix(\pre_j)$.
Then $\fix(\pre_k) \sub \fix(\pre_j)$ is immediate.  
Since this was the only modification to $\coll_n$, condition~(\ref{convlemma:coll}) follows for other indices by the inductive hypothesis. 

\emph{Case 4}: There is a transition $\pair_n \to \pair_{n + 1}$ by adding a vertex to $\forest_n$ after processing the collision digraph $G$ on line~\ref{alg2:forest} of subroutine~\ref{infcomb}.
Note that $\coll_n = \coll_{n + 1}$. 
Clearly the transition from $\forest_n$ to $\forest_{n + 1}$ is of the form required in lemma~\ref{lemma:forest}, so $\forest_{n + 1}$ is still a rooted forest.  
We will show that the construction of $G$ ensures that both conditions of the lemma hold for $\forest_{n + 1}$.
Let $(\forest',\coll')$ be the value of the pair $\pair$ at line~\ref{alg2:forest} when we begin this construction.
Then, by (strong) induction, $\forest'$ is a problem forest for $\act$ and the intersection indices $\coll$ satisfy condition~\ref{convlemma:coll}. 

First, we characterize the edges of $G = (V,E)$. 
Consider $\pre_i \in \acto$ at initialization of the subroutine and $j \in \coll(i)$ with $w_k = r(w_j)$ and $w_{\ell} = r(w_i)$.
Since $\forest'$ is a problem tree, by lemma~\ref{lemma:forest2} above $\fix(\pre_i) \sub \fix(\pre_{\ell})$ and $\fix(\pre_j) \sub \fix(\pre_k)$.
By the inductive hypothesis, $\fix(\pre_{\ell}) \sub \fix(\pre_j)$. 
Putting this together, we have $\fix(\pre_i) \sub \fix(\pre_k)$ for every $(v_i, v_k) \in E$.

Now consider a strong component $\strongcomp_j$ (after removing singletons $\{v_i\}$ for $\pre_i \not \in \acto$). 
For $v_i,v_k \in \strongcomp_j$, there are directed paths $v_i \to v_k$ and $v_k \to v_i$. 
Then, by the correspondence between directed edges and fixed point inclusion shown above, we have $\fix(\pre_i) \sub \fix(\pre_k)$ and $\fix(\pre_k) \sub \fix(\pre_i)$, so $\fix(\pre_i) = \fix(\pre_k)$ for all $v_i, v_k \in \strongcomp_j$. 
Consider $w(\pre)$ added to $\forest_n$ while processing strong component $j$. 
Let $v_k, v_{\ell} \in \strongcomp_j$, with $r(w_k) = w_{k'}$ and $r(w_{\ell}) = w_{\ell'}$.
By induction $\forest = \forest_n$ is a problem forest, so we can compute, using lemma~\ref{lemma:forest2} above, $\fix(\pre_{\ell'}) = \fix(\pre_{\ell}) = \fix(\pre_k) = \fix(\pre_{k'})$. 
This shows that defining condition~(\ref{def:forest2}) above holds for $\forest_{n + 1}$ by induction, since the vertices in $c(w(\pre))$ are only new children added during the change $\forest_n \to \forest_{n + 1}$.  

For the final defining condition of a problem forest, consider adding $\pre$ as above and let $v_i \in \strongcomp_j$. 
Clearly $i \in \reach_j$, so that $i \in K_j$. 
Then $\pre \peq \pre_i$, so $\fix(\pre) \sub \fix(\pre_i)$. 
If $v_k \in \reach_j$, then there is a directed path $v_i \to v_k$, so $\fix(\pre_i) \sub \fix(\pre_k)$ by the observations above. 
In particular, $\fix(\pre_i) \peq \pre_k$ for all $k \in K_j$. 
Then $\fix(\pre_i) \peq \pre = \min_{k \in K_j} \pre_k$, so that $\fix(\pre_i) \sub \fix(\pre)$, and we are done. 

Condition~(\ref{convlemma:coll}) of the lemma follows automatically as discussed in case 3, since we have not modified any collision index $\coll(i)$. 

This exhausts all cases, so the proof is complete. 








\eprf 






\subsubsection{Notes}

AGENDA
work through key lemma with new lemma 
prove main result


change root proof to root before processed? 
can we stick with the single index thing? 
rather than doing that, we just add problems $\pre_i$, where we may have $\pre_j = \pre_i$ for $j \not = i$, seems bad...
add new statement to key tree lemma
break decreasing cone statement off into definition of problem tree? 
this gives a levels type of argument 
formalize acto root statement
prove a single-processing lemma 
break digraph construction off into a visual lemma? 
formalize acto root statement 
first prove, then go correct disjointness assumption
add note about roots after subroutine 1 
look for errors based on assuming that sub2 runs directly after sub1
add tree lemma references
solve the tree issue
ugh, we're not actually adding to root set in subroutine 2 
Let's show that all modifications are of form
    adding an element and joining root set elements to this as children 
    clear by edge and connectedness criterion that this gives a forest of the required form 
how do we know that active points are roots? - this also needs to be shown
proof of problem forest
make sure to prove this actually comes out as a tree - disjointness is the key part
what is the full ind hyp I will need to prove construction 
what form does the induction take / indices
need to include indices in induction
is there something super-formally wrong with doing induction like this? 
add indices statement to proof tagline
use equivalent condition for a tree that it is connected with n-1 vertices
adding children who have no parents - should be fine 
cut out some ugly things by resolving graph on the parents? 

if j = r(i), then prej <  prek for k in I(i) [holds for any index set, always, and any referenced points 
then correct construction from algorithm 1 states implies correct chain 
can we do induction on states separately? 
by inspection, clear that all of Q is represented on F

add technical lemma about how each problem is only executed once - easy enough from removal arguments****
add a formal rooted tree definition to forest 
check rooted tree lemma 
add a reference - graph theory and its applications gross, yellen
maybe global vars help us with induction e.g. make qo global 
see if we can build collision indices all in algo 1 
note that some for loops can be reduced, still arbitrary
show that pinv is an extension of child function 
Explain why this infcomb stage is iterative 
can you cross a new cone on collation iteration? 
define intersection set properly
prealloc notation is not exactly correct
add something above about how the algorithm makes use of 4 geometric facts 
add a note about complexity 
add a note about novel things from above
put in a max based removal step for Q
add a note about how pref is well-defined 
deal with this global tree and Q nonsense
eff result that nothing is ever added twice
maybe we should formally define a collision digraph 
should I add rep Q to defn of a problem forest? 

what additional eff gains do people care about? 

AGENDA
correctness is maintained - crucial 
just prove that while loop terminates 
look at internals of collision dynamics

the reason is, we want consistency for redundant problems i.e. those problems that cannot have moved 

\subsubsection{Discussion}
As can be seen, the construction above is very general, requiring only that the fixed points we want to find are bounded by a lattice, and that we have a way to initialize new problems on sublattices whose solution sets bound the solution set of our original problem. 

\section{Finding All Nash Equilibria in Games with Strategic Complements}
\subsection{Model and Notation}





\section{extra text} 

---------------trying to prove w(pref) has no up-edges in sub1 
Since any such $\pre$ is processed at most once by the first statement of the lemma, by inspection the only possible addition of an edge to $\forest$ naming $\pre$ is of the form $(w(\pre^f),\pre)$ with $c(w(\pre^f)) = w(\pre)$.  
We claim that no directed edge of the form $(w', w(\pre^f))$ is ever added to $\forest$ during the subroutine. 
A directed edge $(w',w_j)$ is added only if $w_j \in \acto$ at some point during the algorithm. 
Let $\pre \in \acto$ and $\pre_j = \pre^f$. 
Then, by the monotonicity of the Tarski sequence for $\lag \pre \rag$, $\pre^f = \pre_j \peq \pre$.

Specifically, we show that our algorithm also gives a faster method of finding all Nash equilibria in games with strategic complementarities (GSC), extending the original work in Echenqiue xxxxx.  
We also show how a version of the cone geometry approach can be used to find all \emph{stable} matchings in the contracts model of Hatfield and Milgrom xxxx.  

Note that the set of preallocations $\prealloc$ is a finite product set endowed with a product order.
Thus, we can identify $\prealloc$ with a grid in Euclidena space, where, for instance, $\pre \in \prealloc$ corresponds to a point in $|A|$ dimensional Euclidean space with components given by the rank of $\pre(a)$ under the order $\su_a$. 

Our approach is very general, relying only on the lattice structure of the problem and the ability to define subproblems whose solutions contain the original points of interest.  

Therefore, by isotonicity of $T^2$, we have $T^4 \pre^* \pe \T^2 \pre^*$ and so on, so, by iterating $T^2$, we obtain a monotonically decreasing sequence, which, by finiteness of $\prealloc$ converges to a fixed point, which we call $\toppre$.
By a standard argument \footnote{insert standard argument}, $\toppre$ is the largest fixed point 

Fix $\acto^{0} = \acto$ at the beginning of 
Since we only add points to $\acto$ in line~\ref{alg1:addq}, we have $\actacum = \actocum$. 
By the proof above, $\acta = \emptyset$ when the subroutine terminates, so every $\pre \in \actacum$ is removed from $\acta$ at some point before termination.



Finally, we show that the outer while loop at line~\ref{alg3:while1} terminates.
Set $\acto^r \leftarrow InformationAcquire(\acto)$. 
We claim that if $\acto \not = \emptyset$, then $d(\acto^r) < d(\acto)$. 
Consider subroutine~\ref{infac} with input $\acto$. 
Let $\actacum$ be the collection of points ever added to $\acta$ and similarly for $\actocum$, which includes the initial value of $\acto$. 
We claim that no preallocation is ever added to $\acta$ more than once. 
Equivalently, since $\acta = \emptyset$ when the algorithm terminates, every point in $\actacum$ is removed from $\acta$ exactly once. 
Suppose not, and $\pre$ is added more than once. 
Consider the first time $\pre$ is added. 
If this is at line~\ref{alg1:add1}, then $\pre \in \acto \sub \act$.
Otherwise, this is at line~\ref{alg1:addq}, so we also add $\pre$ to $\act$.
Now consider the second time $\pre$ is added 
Then $\pre$ is added at line~\ref{alg1:addq}, $\pre = \pre' - e_i$ for some $1 \leq i \leq m$.


By work above, subroutine~\ref{infac} terminates.
Note that the operations in the for loop at line~\ref{alg1:for2} is deterministic and the removal code at lines~\ref{alg1:stop1} and~\ref{alg1:stop2} is mutually exclusive. 

\item For all $\pre_i \in \acto$, $w_i$ is a root of some tree in the forest.

\item If $k \in \coll(i)$ for some $i \in \mathcal{I}$ and $w_j = r(w_i)$, then $\pre_j \peq \pre_k$ \label{lemma:coll}

We show that $r(w_i) = w_i$. 
To do this, we show a result about the structure of $\forest'$, which will also be useful for case 4. 

Note first that, for any $\pre_{\ell} \in \actt$, we have $c(w_{\ell}) = \{r(w_k): v_k \in \strongcomp_j\}$ for some strong component $\strongcomp_j$. 
By disjointness of $\{\strongcomp_j\}_{j \in \mathcal{J}}$, the sets of children $c(w_{\ell})$ above are disjoint for all indices $\ell$. %not totally correct
Let $\forest'$ be the problem forest at the beginning of the for loop at line~\ref{alg2:for2}
Then, during this loop, each $w_{\ell}$ is added as a root joining a collection of disjoint trees in $\forest'$. 
Thus, $w_{\ell} \in \roott(F_n)$ for all $\pre_{\ell} \in \actt$, so this holdsfor $\ell = i$ in particular, and $r(w_i) = w_i$. 
We have, by the case assumption, $\pre_i \peq \pre_j \in \act$ for some $j$, so  condition~(\ref{convlemma:coll}) holds for $j \in \coll(i)$. 
Since the problem forest is unchanged, the conclusion for all other indices holds by the inductive hypothesis. 

deal with case where w(pre) is already in 
adding new? 
how do we know we are actually adding a vertex not yet in the forest....
same problem as above

Moreover, by disjointness, adding $w(\pre_j)$ to $\forest$ does not change any of the other root sets $\{r(w_k): v_k \in \strongcomp_{j'}\}$ associated with components $\strongcomp_{j'} \not = \strongcomp_j$. 
If an edge of the form $(w',w(\pre))$ is added during this process, then it must be that $w(\pre) \in \{r(w_k): v_k \in \strongcomp_{j'}\}$. 

This is impossible, since the root set $\{r(w_k): v_k \in \strongcomp_{j'}\}$ is unchanged until 

Fix $j' \not = j_1$, then, by disjointness of root sets $\{r(w_k): v_k \in \strongcomp_j\}$, adding $w(\pre_1)$ to $\forest'$ does not change any of the other root sets. 
Assume inductively 

$\{r(w_k): v_k \in \strongcomp_j\}$

By construction, $w(\pre)$ is added as a root, since its children are all roots (as in lemma~\ref{lemma:forest}). 
If $w(\pre) \not \in \roott(\forest'')$, then an edge of the form $(w',w(\pre))$ must have been added later during this for loop.  


------- proof about well definition 
We show that line~\ref{alg1:addq} above is well defined. 
That is, if we add a problem $\pre_j = \pre - e_i$, it must \emph{not} be part of the forest already. 
Suppose that this is not the case and $w_j \in \forest$ when we try to add it.
Then we condition on when $w_j$ was added. 
If $w_j$ was added during subroutine~\ref{infcomb}


------ need to figure out how to do this








\end{document}


